{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1quu_S8CBoF_9_KC4WXNgE3U4rRdMV-0n",
      "authorship_tag": "ABX9TyM8TYfTuqPfUGy07gUVIcVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/github-rokon/CVPR/blob/main/Face_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow pyheif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjknnjuzPkLm",
        "outputId": "5ee10e89-b684-4e30-cd28-70d5cfbb268f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: pyheif in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyheif) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->pyheif) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcfSqMBuJ1n-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "\n",
        "\n",
        "import pyheif\n",
        "import cv2\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOGi0jjqKrr4",
        "outputId": "9ec5212d-e0ae-41e0-ac94-e88580058e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/Dataset'"
      ],
      "metadata": {
        "id": "JMkldW68K1MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ],
      "metadata": {
        "id": "i5QmT3C5dWHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKpmI0dvlsNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_heic_image(image_path):\n",
        "    # Read HEIC image using pyheif\n",
        "    heif_file = pyheif.read(image_path)\n",
        "    # Convert it into a format that Pillow can work with\n",
        "    image = Image.frombytes(\n",
        "        heif_file.mode,\n",
        "        heif_file.size,\n",
        "        heif_file.data,\n",
        "        \"raw\",\n",
        "        heif_file.mode,\n",
        "        heif_file.stride,\n",
        "    )\n",
        "    return image"
      ],
      "metadata": {
        "id": "UStnELZkQCJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Load dataset with resizing images to the same dimensions\n",
        "def load_dataset(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_names = os.listdir(dataset_path)\n",
        "\n",
        "    for index, name in enumerate(tqdm(label_names)):\n",
        "        folder_path = os.path.join(dataset_path, name)\n",
        "        for image_name in os.listdir(folder_path):\n",
        "            image_path = os.path.join(folder_path, image_name)\n",
        "\n",
        "            # Check if the file is a HEIC or regular image\n",
        "            if image_name.lower().endswith('.heic'):\n",
        "                try:\n",
        "                    image = load_heic_image(image_path)  # Load HEIC image\n",
        "                    image = np.array(image)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading HEIC image: {image_path} - {e}\")\n",
        "                    continue\n",
        "            else:\n",
        "                image = cv2.imread(image_path)  # Load non-HEIC image with OpenCV\n",
        "\n",
        "            if image is not None:  # Check if the image was successfully loaded\n",
        "                image = cv2.resize(image, (224, 224))  # Resize to 224x224\n",
        "                images.append(image)\n",
        "                labels.append(index)\n",
        "            else:\n",
        "                print(f\"Warning: Could not load image {image_path}\")\n",
        "\n",
        "    images = np.array(images, dtype='float32') / 255.0  # Normalize\n",
        "    labels = np.array(labels)\n",
        "    labels = to_categorical(labels, num_classes=len(label_names))\n",
        "\n",
        "    return images, labels, label_names # Return the images and labels\n",
        "\n",
        "# Example usage\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/Dataset' # Replace with the actual path to your dataset\n",
        "images, labels, label_names = load_dataset(dataset_path) # Call the function and assign the returned values\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTEsinBspJel",
        "outputId": "bfdc113e-963c-4280-c25a-5c3f4b48f74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/56 [00:00<00:49,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not load image /content/drive/MyDrive/Colab Notebooks/Dataset/Rokon/Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 45/56 [00:45<00:02,  3.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading HEIC image: /content/drive/MyDrive/Colab Notebooks/Dataset/SUN/IMG_6943.HEIC - Input is not a HEIF/AVIF file\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56/56 [00:50<00:00,  1.11it/s]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "\n",
        "# Test different camera indices\n",
        "for i in range(5):  # Try indices 0 to 4\n",
        "    cap = cv2.VideoCapture(i)\n",
        "    if cap.isOpened():\n",
        "        print(f\"Camera found at index {i}\")\n",
        "        cap.release()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i99FpvgnmFEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CNN model\n",
        "def build_model(num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax'),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(num_classes=len(label_names))\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "GCnfhrUxQ_Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPFG3rPxRIy8",
        "outputId": "4a397c7f-f85d-46d4-bd33-96b819be1999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 7s/step - accuracy: 0.0267 - loss: 4.2143 - val_accuracy: 0.0230 - val_loss: 4.0004\n",
            "Epoch 2/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 7s/step - accuracy: 0.1201 - loss: 3.7108 - val_accuracy: 0.1379 - val_loss: 3.7978\n",
            "Epoch 3/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 0.3894 - loss: 3.1916 - val_accuracy: 0.2529 - val_loss: 3.4023\n",
            "Epoch 4/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 7s/step - accuracy: 0.5275 - loss: 2.3957 - val_accuracy: 0.3333 - val_loss: 2.8925\n",
            "Epoch 5/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 7s/step - accuracy: 0.7283 - loss: 1.6130 - val_accuracy: 0.2644 - val_loss: 3.1442\n",
            "Epoch 6/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 7s/step - accuracy: 0.8246 - loss: 0.9986 - val_accuracy: 0.3793 - val_loss: 2.6830\n",
            "Epoch 7/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 7s/step - accuracy: 0.8997 - loss: 0.5539 - val_accuracy: 0.3563 - val_loss: 2.6129\n",
            "Epoch 8/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 7s/step - accuracy: 0.9716 - loss: 0.2871 - val_accuracy: 0.3793 - val_loss: 2.6002\n",
            "Epoch 9/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 7s/step - accuracy: 0.9829 - loss: 0.1216 - val_accuracy: 0.4368 - val_loss: 2.5870\n",
            "Epoch 10/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0441 - val_accuracy: 0.4368 - val_loss: 2.5479\n",
            "Epoch 11/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7s/step - accuracy: 0.9978 - loss: 0.0246 - val_accuracy: 0.4023 - val_loss: 2.9033\n",
            "Epoch 12/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0216 - val_accuracy: 0.4598 - val_loss: 2.7663\n",
            "Epoch 13/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0073 - val_accuracy: 0.4598 - val_loss: 2.6597\n",
            "Epoch 14/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.4828 - val_loss: 2.6157\n",
            "Epoch 15/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.4828 - val_loss: 2.6685\n",
            "Epoch 16/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.4828 - val_loss: 2.6824\n",
            "Epoch 17/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.4828 - val_loss: 2.6941\n",
            "Epoch 18/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.4828 - val_loss: 2.7264\n",
            "Epoch 19/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.4713 - val_loss: 2.7507\n",
            "Epoch 20/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.4713 - val_loss: 2.7574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cB3mVTWx-oFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a909GUNXdA48"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "# Function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "    return img\n",
        "\n",
        "# Function to convert OpenCV Rectangle bounding box image into base64 byte string\n",
        "def bbox_to_bytes(bbox_array):\n",
        "    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    bbox_PIL.save(iobuf, format='png')\n",
        "    bbox_bytes = 'data:image/png;base64,{}'.format(str(b64encode(iobuf.getvalue()), 'utf-8'))\n",
        "    return bbox_bytes\n",
        "\n",
        "# Initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "oQT0QhYg-vDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Function to preprocess the image for recognition\n",
        "def preprocess_for_recognition(img):\n",
        "    img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img_resized = img_resized / 255.0\n",
        "    return np.expand_dims(img_resized, axis=0)\n",
        "\n",
        "# Function to mark attendance\n",
        "def mark_attendance(name):\n",
        "    print(f\"Marking attendance for {name}\")\n",
        "\n",
        "# Start streaming video from webcam\n",
        "video_stream()\n",
        "\n",
        "# Label for video\n",
        "label_html = 'Capturing...'\n",
        "\n",
        "# Initialize bounding box to empty\n",
        "bbox = ''\n",
        "\n",
        "# Dictionary to keep track of attendance\n",
        "attendance_record = {}\n",
        "\n",
        "# Delay between attendance markings (in seconds)\n",
        "attendance_delay = 5\n",
        "\n",
        "confidence_threshold = 0.7\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_roi = img[y:y+h, x:x+w]\n",
        "\n",
        "        face_for_recognition = preprocess_for_recognition(face_roi)\n",
        "\n",
        "        prediction = model.predict(face_for_recognition)\n",
        "        predicted_class_index = np.argmax(prediction)\n",
        "        confidence = prediction[0][predicted_class_index]\n",
        "        predicted_class = CATEGORIES[predicted_class_index]\n",
        "\n",
        "        if confidence > confidence_threshold and predicted_class not in attendance_record:\n",
        "            mark_attendance(predicted_class)\n",
        "            attendance_record[predicted_class] = True\n",
        "\n",
        "            time.sleep(attendance_delay)\n",
        "\n",
        "        bbox_array = cv2.rectangle(bbox_array, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        cv2.putText(bbox_array, f'{predicted_class} ({confidence:.2f})', (x + 2, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(int) * 255\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    bbox = bbox_bytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nEn2xqwYGouT",
        "outputId": "c8a26e62-8077-4b8c-f09f-289416209764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML =\n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "\n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "\n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "\n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "\n",
              "      return {'create': preShow - preCreate,\n",
              "              'show': preCapture - preShow,\n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def real_time_recognition(model, label_names):\n",
        "    # Initialize the webcam (use 0 as the parameter to select the default webcam)\n",
        "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # For Windows\n",
        "\n",
        "    # Check if the webcam is opened correctly\n",
        "    if not cap.isOpened():\n",
        "        raise IOError(\"Cannot open webcam\")\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()  # Read a frame from the webcam\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to grayscale (Haar cascades work with grayscale images)\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Draw a rectangle around each face\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "        # Extract the face region\n",
        "        face_region = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Preprocess the face region for prediction\n",
        "        face = cv2.resize(face_region, (224, 224))\n",
        "        face = np.expand_dims(face, axis=0)\n",
        "        face = face / 255.0\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(face)\n",
        "        predicted_class = label_names[np.argmax(prediction)]\n",
        "\n",
        "        # Display the predicted class label\n",
        "        cv2.putText(frame, predicted_class, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Show the frame with face rectangles and predicted class labels\n",
        "    cv2.imshow('Real-time Face Recognition', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "\n",
        "    # Release the webcam and close all OpenCV windows\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Assuming 'model' is your trained model and 'label_names' is a list of class names\n",
        "# real_time_recognition(model, label_names)"
      ],
      "metadata": {
        "id": "gznKCyWNXgSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}