{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1quu_S8CBoF_9_KC4WXNgE3U4rRdMV-0n",
      "authorship_tag": "ABX9TyPnm4VWQrfoJKa7JcZZamg/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/github-rokon/CVPR/blob/main/Face_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow pyheif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjknnjuzPkLm",
        "outputId": "14569104-0b28-43c0-9274-56f41f25785a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: pyheif in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyheif) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->pyheif) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CW60c3ZJjkzP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vcfSqMBuJ1n-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "\n",
        "\n",
        "import pyheif\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOGi0jjqKrr4",
        "outputId": "a79d1385-6d6d-4db8-a770-41ca3453546e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/Dataset'"
      ],
      "metadata": {
        "id": "JMkldW68K1MQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ],
      "metadata": {
        "id": "i5QmT3C5dWHM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKpmI0dvlsNA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_heic_image(image_path):\n",
        "    # Read HEIC image using pyheif\n",
        "    heif_file = pyheif.read(image_path)\n",
        "    # Convert it into a format that Pillow can work with\n",
        "    image = Image.frombytes(\n",
        "        heif_file.mode,\n",
        "        heif_file.size,\n",
        "        heif_file.data,\n",
        "        \"raw\",\n",
        "        heif_file.mode,\n",
        "        heif_file.stride,\n",
        "    )\n",
        "    return image"
      ],
      "metadata": {
        "id": "UStnELZkQCJF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "source": [
        "# Load dataset with resizing images to the same dimensions\n",
        "def load_dataset(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_names = os.listdir(dataset_path)\n",
        "\n",
        "    for index, name in enumerate(tqdm(label_names)):\n",
        "        folder_path = os.path.join(dataset_path, name)\n",
        "        for image_name in os.listdir(folder_path):\n",
        "            image_path = os.path.join(folder_path, image_name)\n",
        "\n",
        "            # Check if the file is a HEIC or regular image\n",
        "            if image_name.lower().endswith('.heic'):\n",
        "                try:\n",
        "                    image = load_heic_image(image_path)  # Load HEIC image\n",
        "                    image = np.array(image)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading HEIC image: {image_path} - {e}\")\n",
        "                    continue\n",
        "            else:\n",
        "                image = cv2.imread(image_path)  # Load non-HEIC image with OpenCV\n",
        "\n",
        "            if image is not None:  # Check if the image was successfully loaded\n",
        "                image = cv2.resize(image, (224, 224))  # Resize to 224x224\n",
        "                images.append(image)\n",
        "                labels.append(index)\n",
        "            else:\n",
        "                print(f\"Warning: Could not load image {image_path}\")\n",
        "\n",
        "    images = np.array(images, dtype='float32') / 255.0  # Normalize\n",
        "    labels = np.array(labels)\n",
        "    labels = to_categorical(labels, num_classes=len(label_names))\n",
        "\n",
        "    return images, labels, label_names # Return the images and labels\n",
        "\n",
        "# Example usage\n",
        "dataset_path = '/content/drive/MyDrive/Colab Notebooks/Dataset' # Replace with the actual path to your dataset\n",
        "images, labels, label_names = load_dataset(dataset_path) # Call the function and assign the returned values\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTEsinBspJel",
        "outputId": "d17615ad-63c8-4d5b-94d4-e31aabc67bbe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/56 [00:00<00:40,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not load image /content/drive/MyDrive/Colab Notebooks/Dataset/Rokon/Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 45/56 [00:41<00:03,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading HEIC image: /content/drive/MyDrive/Colab Notebooks/Dataset/SUN/IMG_6943.HEIC - Input is not a HEIF/AVIF file\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 56/56 [00:49<00:00,  1.13it/s]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "\n",
        "# Test different camera indices\n",
        "for i in range(5):  # Try indices 0 to 4\n",
        "    cap = cv2.VideoCapture(i)\n",
        "    if cap.isOpened():\n",
        "        print(f\"Camera found at index {i}\")\n",
        "        cap.release()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i99FpvgnmFEv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CNN model\n",
        "def build_model(num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax'),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(num_classes=len(label_names))\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "GCnfhrUxQ_Sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91595805-4fc3-4382-8507-0911038eb0a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPFG3rPxRIy8",
        "outputId": "7c1accf6-83dc-4466-d135-41a776f72cc8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 0.0244 - loss: 4.3080 - val_accuracy: 0.0575 - val_loss: 3.9558\n",
            "Epoch 2/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 7s/step - accuracy: 0.1360 - loss: 3.8091 - val_accuracy: 0.1264 - val_loss: 3.9029\n",
            "Epoch 3/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 0.2493 - loss: 3.4859 - val_accuracy: 0.1839 - val_loss: 3.7307\n",
            "Epoch 4/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 0.4620 - loss: 2.7714 - val_accuracy: 0.1839 - val_loss: 3.5732\n",
            "Epoch 5/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 0.6377 - loss: 2.0840 - val_accuracy: 0.2184 - val_loss: 3.2854\n",
            "Epoch 6/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 7s/step - accuracy: 0.7726 - loss: 1.3700 - val_accuracy: 0.3448 - val_loss: 2.8462\n",
            "Epoch 7/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 7s/step - accuracy: 0.9204 - loss: 0.6412 - val_accuracy: 0.2644 - val_loss: 3.2362\n",
            "Epoch 8/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 0.9405 - loss: 0.4089 - val_accuracy: 0.3793 - val_loss: 2.6630\n",
            "Epoch 9/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 0.9824 - loss: 0.1768 - val_accuracy: 0.3793 - val_loss: 2.9164\n",
            "Epoch 10/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 0.9858 - loss: 0.0995 - val_accuracy: 0.3908 - val_loss: 2.8763\n",
            "Epoch 11/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0343 - val_accuracy: 0.4023 - val_loss: 2.8969\n",
            "Epoch 12/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0141 - val_accuracy: 0.4023 - val_loss: 2.8331\n",
            "Epoch 13/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.4253 - val_loss: 2.8468\n",
            "Epoch 14/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0062 - val_accuracy: 0.4138 - val_loss: 2.7695\n",
            "Epoch 15/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.4368 - val_loss: 2.8447\n",
            "Epoch 16/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.4368 - val_loss: 2.9177\n",
            "Epoch 17/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.4483 - val_loss: 2.9477\n",
            "Epoch 18/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.4483 - val_loss: 2.9683\n",
            "Epoch 19/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.4483 - val_loss: 2.9898\n",
            "Epoch 20/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.4483 - val_loss: 3.0345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def real_time_recognition(model, label_names):\n",
        "    # Initialize the webcam (use 0 as the parameter to select the default webcam)\n",
        "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # For Windows\n",
        "\n",
        "    # Check if the webcam is opened correctly\n",
        "    if not cap.isOpened():\n",
        "        raise IOError(\"Cannot open webcam\")\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()  # Read a frame from the webcam\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to grayscale (Haar cascades work with grayscale images)\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Draw a rectangle around each face\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "        # Extract the face region\n",
        "        face_region = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Preprocess the face region for prediction\n",
        "        face = cv2.resize(face_region, (224, 224))\n",
        "        face = np.expand_dims(face, axis=0)\n",
        "        face = face / 255.0\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(face)\n",
        "        predicted_class = label_names[np.argmax(prediction)]\n",
        "\n",
        "        # Display the predicted class label\n",
        "        cv2.putText(frame, predicted_class, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Show the frame with face rectangles and predicted class labels\n",
        "    cv2.imshow('Real-time Face Recognition', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "\n",
        "    # Release the webcam and close all OpenCV windows\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Assuming 'model' is your trained model and 'label_names' is a list of class names\n",
        "# real_time_recognition(model, label_names)"
      ],
      "metadata": {
        "id": "gznKCyWNXgSS"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}